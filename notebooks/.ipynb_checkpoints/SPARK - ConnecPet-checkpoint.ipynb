{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns;sns.set(context='notebook', palette='pastel', style='darkgrid')\n",
    "%matplotlib inline\n",
    "from dtw import dtw\n",
    "import math\n",
    "import ctypes\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf , array\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from pyspark.sql.types import DataType\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.types import  FloatType\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = spark.read.load(\"/home/datasciencemlops/Documents/DataScience/Connect/ConnectPET/data/db_merged.parquet/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:          10092        3042        5120          61        1929        6630\r\n",
      "Swap:           974           0         974\r\n"
     ]
    }
   ],
   "source": [
    "!free -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[index: bigint, activity: string, collar_id: bigint, u_id_coleira: string, gx: double, gy: double, gz: double, ax: bigint, ay: bigint, az: bigint, temp: bigint, time_stamp: bigint, pet_id: bigint, size: string, race: string, age: string, genre: string, owner_id: bigint, lat: string, long: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conf = (SparkConf().setMaster(\"localHost\").setAppName(\"recommender\").set(\"spark.executor.memory\", \"9g\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Projeto ConnetPet-SPARK\") \\\n",
    "   .config(\"spark.executor.memory\", \"9gb\") \\\n",
    "   .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_spark[df_spark['collar_id'] == 1571851735]\n",
    "df = df.toPandas()\n",
    "df['data_'] = pd.to_datetime(df['time_stamp'],unit='ms')\n",
    "df.sort_values(by=['data_'],inplace=True)\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_values(x):\n",
    "    return  (x * 32) / 65536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter_valores_xyx = udf(lambda x : convert_values(x),FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.withColumn('ax_',converter_valores_xyx('ax'))\\\n",
    "  .withColumn('ay_',converter_valores_xyx('ay'))\\\n",
    "  .withColumn('az_',converter_valores_xyx('az'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.withColumn('a_xyz_',(F.sqrt(df2['ax_']**2 +F.sqrt(df2['ay_']**2 + F.sqrt(df2['az_']**2)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"df_sample = df3.withColumn('dist',lit(np.NAN))\n",
    "\n",
    "jump = 10\n",
    "max_len = df_sample.count()\n",
    "#max_len = 3\n",
    "count = 0\n",
    " \n",
    "manhattan_distance = lambda x, y: np.abs(x - y)\n",
    "\n",
    "\n",
    "\n",
    "df_sample = df3.where(col('level_0').between(0,4))\n",
    "\n",
    "while jump + count < max_len:\n",
    "    \n",
    "    # Gets Current time window\n",
    "    df_current = df_sample.where(col('level_0').between(count,count + jump))\n",
    "    \n",
    "    # Gets next time window\n",
    "    df_next = df_sample.where(col('level_0').between(count + jump,jump * 2 + count))\n",
    "    \n",
    "    # Makes sure the two windows are in the same size\n",
    "    if df_current.count() < jump or df_next.count() < jump:\n",
    "        continue \n",
    "    \n",
    "    # calculates similarity between the two sequences\n",
    "    d ,_, _,_ = dtw(df_current.select('a_xyz_').toPandas().values, df_next.select('a_xyz_').toPandas().values,\n",
    "    dist=manhattan_distance)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Calculates the variance of the current wave and adds to a column\n",
    "    df_sample.loc[df_current.index, 'variance'] = np.var(df_current['a_xyz'].values)\n",
    "    \n",
    "    # Creates a new column with the similarity\n",
    "    \n",
    "    update_func = (F.when(F.col('dist') == np.NaN, d)\n",
    "                .otherwise(F.col('dist')))\n",
    "    df_final = df_sample.withColumn('dist', update_func)\n",
    "    #\n",
    "    count = count + jump  \n",
    "    print(count)\n",
    "%%timeit\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
