{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns;sns.set(context='notebook', palette='pastel', style='darkgrid')\n",
    "%matplotlib inline\n",
    "from dtw import dtw\n",
    "import math\n",
    "import ctypes\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf , array\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from pyspark.sql.types import DataType\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.types import  FloatType\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark --num-executors 5 --driver-memory 2g --executor-memory 2g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install matplotlib\n",
    "#!pip3 install dask\n",
    "#!pip3 install toolz\n",
    "#!pip3 install dask[array] --upgrade\n",
    "#!pip3 install dask[complete] distributed --upgrade\n",
    "#!pip install dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = spark.read.load(\"/home/datasciencemlops/Documents/DataScience/Connect/ConnectPET/data/db_merged.parquet/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[index: bigint, activity: string, collar_id: bigint, u_id_coleira: string, gx: double, gy: double, gz: double, ax: bigint, ay: bigint, az: bigint, temp: bigint, time_stamp: bigint, pet_id: bigint, size: string, race: string, age: string, genre: string, owner_id: bigint, lat: string, long: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[index: bigint, activity: string, collar_id: bigint, u_id_coleira: string, gx: double, gy: double, gz: double, ax: bigint, ay: bigint, az: bigint, temp: bigint, time_stamp: bigint, pet_id: bigint, size: string, race: string, age: string, genre: string, owner_id: bigint, lat: string, long: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Projeto ConnetPet-SPARK\") \\\n",
    "   .config(\"spark.executor.memory\", \"5gb\") \\\n",
    "   .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df_spark[df_spark['collar_id'] == 1571851735]\n",
    "df = df_spark[df_spark['collar_id'] == 1573049556516]\n",
    "df = df.toPandas()\n",
    "df['data_'] = pd.to_datetime(df['time_stamp'],unit='ms')\n",
    "df.sort_values(by=['data_'],inplace=True)\n",
    "df = df.reset_index()\n",
    "#df = df[df['activity'] != 'NO LABEL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_values(x):\n",
    "    return  (x * 32) / 65536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter_valores_xyx = udf(lambda x : convert_values(x),FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.withColumn('ax_',converter_valores_xyx('ax'))\\\n",
    "  .withColumn('ay_',converter_valores_xyx('ay'))\\\n",
    "  .withColumn('az_',converter_valores_xyx('az'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.withColumn('a_xyz_',(F.sqrt(df2['ax_']**2 +F.sqrt(df2['ay_']**2 + F.sqrt(df2['az_']**2)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df3.withColumn('dist',lit(np.NAN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col = split(df_sample['data_'], ' ')\n",
    "df_sample = df_sample.withColumn('Data', split_col.getItem(0))\n",
    "df_sample = df_sample.withColumn('Hora', split_col.getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\njump = 10\\nmax_len = df_sample.count()\\ncount = 0\\n\\nmanhattan_distance = lambda x, y: np.abs(x - y)\\n\\ndf_sample = df3.where(col('level_0').between(0,4))\\n\\nwhile jump + count < max_len:\\n    \\n    # Gets Current time window\\n    df_current = df_sample.where(col('level_0').between(count,count + jump))\\n    \\n    # Gets next time window\\n    df_next = df_sample.where(col('level_0').between(count + jump,jump * 2 + count))\\n    \\n    # Makes sure the two windows are in the same size\\n    if df_current.count() < jump or df_next.count() < jump:\\n        continue \\n    \\n    # calculates similarity between the two sequences\\n    d ,_, _,_ = dtw(df_current.select('a_xyz_').toPandas().values, df_next.select('a_xyz_').toPandas().values,\\n    dist=manhattan_distance)   \\n    \\n    \\n    # Calculates the variance of the current wave and adds to a column\\n    #df_sample.loc[df_current.index, 'variance'] = np.var(df_current['a_xyz'].values)\\n    \\n    # Creates a new column with the similarity\\n    \\n    update_func = (F.when(F.col('dist') == np.NaN, d)\\n                .otherwise(F.col('dist')))\\n    df_final = df_sample.withColumn('dist', update_func)\\n    #\\n    count = count + jump     \""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "jump = 10\n",
    "max_len = df_sample.count()\n",
    "count = 0\n",
    "\n",
    "manhattan_distance = lambda x, y: np.abs(x - y)\n",
    "\n",
    "df_sample = df3.where(col('level_0').between(0,4))\n",
    "\n",
    "while jump + count < max_len:\n",
    "    \n",
    "    # Gets Current time window\n",
    "    df_current = df_sample.where(col('level_0').between(count,count + jump))\n",
    "    \n",
    "    # Gets next time window\n",
    "    df_next = df_sample.where(col('level_0').between(count + jump,jump * 2 + count))\n",
    "    \n",
    "    # Makes sure the two windows are in the same size\n",
    "    if df_current.count() < jump or df_next.count() < jump:\n",
    "        continue \n",
    "    \n",
    "    # calculates similarity between the two sequences\n",
    "    d ,_, _,_ = dtw(df_current.select('a_xyz_').toPandas().values, df_next.select('a_xyz_').toPandas().values,\n",
    "    dist=manhattan_distance)   \n",
    "    \n",
    "    \n",
    "    # Calculates the variance of the current wave and adds to a column\n",
    "    #df_sample.loc[df_current.index, 'variance'] = np.var(df_current['a_xyz'].values)\n",
    "    \n",
    "    # Creates a new column with the similarity\n",
    "    \n",
    "    update_func = (F.when(F.col('dist') == np.NaN, d)\n",
    "                .otherwise(F.col('dist')))\n",
    "    df_final = df_sample.withColumn('dist', update_func)\n",
    "    #\n",
    "    count = count + jump     \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample.select(\n",
    "    \"*\",\n",
    "    split(\"Hora\", \":\")[0].cast(\"int\").alias(\"hour\"),\n",
    "    split(\"Hora\", \":\")[1].cast(\"int\").alias(\"minute\"),\n",
    "    split(\"Hora\", \":\")[2].cast(\"int\").alias(\"second\")\n",
    ").sort('Hora','second', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      Data|\n",
      "+----------+\n",
      "|2019-11-06|\n",
      "|2019-11-10|\n",
      "|2019-11-15|\n",
      "|2019-11-08|\n",
      "|2019-11-09|\n",
      "|2019-11-07|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample.select('Data').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample.filter(\"Data == '2019-11-06'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[level_0: bigint, index: bigint, activity: string, collar_id: bigint, u_id_coleira: string, gx: double, gy: double, gz: double, ax: bigint, ay: bigint, az: bigint, temp: bigint, time_stamp: bigint, pet_id: bigint, size: string, race: string, age: string, genre: string, owner_id: bigint, lat: string, long: string, data_: timestamp, ax_: float, ay_: float, az_: float, a_xyz_: double, dist: double, Data: string, Hora: string, hour: int, minute: int, second: int]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.withColumn('dist',lit(np.NAN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "jump = 10\n",
    "max_len = df_sample.count()\n",
    "#max_len = 3\n",
    "count = 0\n",
    " \n",
    "manhattan_distance = lambda x, y: np.abs(x - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-d4dcdab3e89c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#df_sample.where(col('level_0').between(0,4)).toPandas()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#df_sample.where(col('index').between(count,count + jump)).toPandas()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"level_0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequalTo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "#df_sample.where(col('level_0').between(0,4)).toPandas()\n",
    "#df_sample.where(col('index').between(count,count + jump)).toPandas()\n",
    "df_sample.filter(col(\"level_0\").equalTo(4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#while jump + count < max_len:\n",
    "#    \n",
    "#    # Gets Current time window\n",
    "#df_current = df_sample.where(col('level_0').between(count,count + jump))\n",
    "#    \n",
    "#    # Gets next time window\n",
    "#df_next = df_sample.where(col('level_0').between(count + jump,jump * 2 + count))\n",
    "#    \n",
    "#    # Makes sure the two windows are in the same size\n",
    "#    if df_current.count() < jump or df_next.count() < jump:\n",
    "#        continue \n",
    "#    \n",
    "#    # calculates similarity between the two sequences\n",
    "#d ,_, _,_ = dtw(df_current.select('a_xyz_').toPandas().values, df_next.select('a_xyz_').toPandas().values,\n",
    "#            dist=manhattan_distance)\n",
    "#    \n",
    "#    \n",
    "#    \n",
    "#    # Calculates the variance of the current wave and adds to a column\n",
    "#    df_sample.loc[df_current.index, 'variance'] = np.var(df_current['a_xyz'].values)\n",
    "#    \n",
    "#    # Creates a new column with the similarity\n",
    "#   \n",
    "#    df_sample.loc[df_current.index, 'dist'] = d\n",
    "#update_func = (F.when(F.col('dist') == np.NaN, d)\n",
    "#            .otherwise(F.col('dist')))\n",
    "#df_sample.withColumn('dist', update_func)\n",
    "\n",
    "#    count = count + jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
