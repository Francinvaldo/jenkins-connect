{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns;sns.set(context='notebook', palette='pastel', style='darkgrid')\n",
    "%matplotlib inline\n",
    "from dtw import dtw\n",
    "import math\n",
    "import ctypes\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf , array\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from pyspark.sql.types import DataType\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.types import  FloatType\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import col\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from fastdtw import fastdtw\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',800)\n",
    "pd.set_option('display.max_rows',800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = spark.read.load(\"/home/datasciencemlops/Documents/DataScience/Connect/ConnectPET/data/db_merged.parquet/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[index: bigint, activity: string, collar_id: bigint, u_id_coleira: string, gx: double, gy: double, gz: double, ax: bigint, ay: bigint, az: bigint, temp: bigint, time_stamp: bigint, pet_id: bigint, size: string, race: string, age: string, genre: string, owner_id: bigint, lat: string, long: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[index: bigint, activity: string, collar_id: bigint, u_id_coleira: string, gx: double, gy: double, gz: double, ax: bigint, ay: bigint, az: bigint, temp: bigint, time_stamp: bigint, pet_id: bigint, size: string, race: string, age: string, genre: string, owner_id: bigint, lat: string, long: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Projeto ConnetPet-SPARK\") \\\n",
    "   .config(\"spark.executor.memory\", \"5gb\") \\\n",
    "   .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df_spark[df_spark['collar_id'] == 1575546080]\n",
    "df = df_spark[df_spark['collar_id'] == 1571851735]\n",
    "df = df.toPandas()\n",
    "df['data_'] = pd.to_datetime(df['time_stamp'],unit='ms')\n",
    "df.sort_values(by=['data_'],inplace=True)\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_values(x):\n",
    "    return  (x * 32) / 65536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter_valores_xyx = udf(lambda x : convert_values(x),FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.withColumn('ax_',converter_valores_xyx('ax'))\\\n",
    "  .withColumn('ay_',converter_valores_xyx('ay'))\\\n",
    "  .withColumn('az_',converter_valores_xyx('az'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.withColumn('a_xyz_',(F.sqrt(df2['ax_']**2 +F.sqrt(df2['ay_']**2 + F.sqrt(df2['az_']**2)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df3.withColumn('dist',lit(np.NAN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col = split(df_sample['data_'], ' ')\n",
    "df_sample = df_sample.withColumn('Data', split_col.getItem(0))\n",
    "df_sample = df_sample.withColumn('Hora', split_col.getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample.select(\n",
    "    \"*\",\n",
    "    split(\"Hora\", \":\")[0].cast(\"int\").alias(\"hour\"),\n",
    "    split(\"Hora\", \":\")[1].cast(\"int\").alias(\"minute\"),\n",
    "    split(\"Hora\", \":\")[2].cast(\"int\").alias(\"second\")\n",
    ").sort('Hora','second', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample.withColumn(\"a_xyz_\",df_sample['a_xyz_'].cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll,dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determines the window of sliding of a 10Hz sensor.\n",
    "from dtaidistance import dtw\n",
    "jump = 10\n",
    "max_len = df_sample.count()\n",
    "#max_len = 40\n",
    "count = 0\n",
    "\n",
    "manhattan_distance = lambda x, y: np.abs(x - y)\n",
    "\n",
    "while jump + count < max_len:\n",
    "    df_current = df_sample.where(col('level_0').between(count,count + jump))\n",
    "\n",
    "    df_ = df_current.groupBy('Data','Hora').agg(F.collect_list(\"a_xyz_\").alias('Data_atual'))\n",
    "    da = df_.select('Data_atual').collect()\n",
    "    dt1 = da[0][0]\n",
    "\n",
    "\n",
    "\n",
    "    df_next = df_sample.where(col('level_0').between(count + jump,jump * 2 + count))\n",
    "    df_1 = df_next.groupBy('Data','Hora').agg(F.collect_list(\"a_xyz_\").alias('proxima_data'))\n",
    "    pd = df_1.select('proxima_data').collect()\n",
    "    dc = pd[0][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    A = np.array(dt1)\n",
    "    B = np.array(dc)\n",
    "\n",
    "    distance, paths = dtw.warping_paths(A, B)\n",
    "    df_final = df_.join(df_1,on=['Data'],how='inner')\n",
    "\n",
    "\n",
    "\n",
    "    df_final = df_final.withColumn(\"distancia\",lit(0.0))      \n",
    "    update_coluna_distancia = (F.when(F.col('distancia') == 0.0 , distance).otherwise(F.col('distancia')))   \n",
    "    df_final = df_final.withColumn('distancia',update_coluna_distancia)\n",
    "\n",
    "\n",
    "    count = count + jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
